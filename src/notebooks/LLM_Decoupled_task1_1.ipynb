{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac27a295",
   "metadata": {},
   "source": [
    "# ðŸ¤– Sistema HÃ­brido de DetecciÃ³n de Sexismo - EXIST2025\n",
    "\n",
    "## Componentes del Sistema\n",
    "\n",
    "### 1. ConexiÃ³n con Decoupled AI (LLM Remote)\n",
    "- **Modelo**: Mistral-Nemo via Decoupled AI\n",
    "- **Uso**: AnÃ¡lisis contextual avanzado para las 3 subtareas EXIST2025\n",
    "\n",
    "### 2. Sistema de Prompts Especializados  \n",
    "- **Subtask 1.1**: DetecciÃ³n binaria de sexismo\n",
    "- **Subtask 1.2**: AnÃ¡lisis de intenciÃ³n (DIRECT/REPORTED/JUDGEMENTAL)\n",
    "- **Subtask 1.3**: CategorizaciÃ³n multilabel (5 categorÃ­as)\n",
    "\n",
    "### 3. Arquitectura HÃ­brida\n",
    "- **Local**: Transformer MDeBERTa optimizado para CPU\n",
    "- **Remoto**: LLM Mistral-Nemo para comprensiÃ³n contextual\n",
    "- **FusiÃ³n**: Ensemble adaptativo basado en confianza\n",
    "\n",
    "> **Nota**: La documentaciÃ³n acadÃ©mica completa estÃ¡ disponible en la estructura LaTeX del TFM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739509f",
   "metadata": {},
   "source": [
    "**USANDO DECOPLED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92a6d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from ollama import Client\n",
    "\n",
    "\n",
    "def prompt_decoupled(query, model='mistral-nemo:latest', context=[]):\n",
    "\n",
    "    host = 'https://ollama01.decoupled.ai'\n",
    "    # host = 'http://localhost:11434'\n",
    "\n",
    "    payload = {\n",
    "        \"prompt\": query,\n",
    "        \"model\": model,\n",
    "        \"stream\": False,\n",
    "        \"context\": context,\n",
    "        \"keep_alive\": \"1h\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        messages = []\n",
    "        message = {}\n",
    "        message['role'] = \"user\" # system, assistant, user\n",
    "        message['content'] = query\n",
    "        messages.append(message)\n",
    "\n",
    "        res = {}\n",
    "\n",
    "        #res['response'], model = decoupled_rest(model, host, messages)\n",
    "        res['response'], model = decoupled_python(model, host, messages)\n",
    "\n",
    "        input_token_count = token_count(query)\n",
    "        output_token_count = token_count(res['response'])\n",
    "\n",
    "        if res['response'] is not None and res['response'] != '':\n",
    "            return res, model, input_token_count, output_token_count\n",
    "        else:\n",
    "            print(\"EXCEPTION RES\")\n",
    "            return None, model, 0, 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"EXCEPTION: \", str(e))\n",
    "        return None, model, 0, 0\n",
    "\n",
    "\n",
    "def decoupled_rest(model, url, payload):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    res = requests.post(url, headers=headers, data=json.dumps(payload), timeout=120)\n",
    "\n",
    "    if res.status_code == 200:\n",
    "        return res.text, model\n",
    "    else:\n",
    "        return None, model\n",
    "\n",
    "\n",
    "def decoupled_python(model, url, messages):\n",
    "\n",
    "    client = Client(host=url, timeout=120)\n",
    "    response = client.chat(model=model, messages=messages, keep_alive=\"168h\")\n",
    "\n",
    "    return response['message']['content'], model\n",
    "\n",
    "\n",
    "def token_count(phrase):\n",
    "    tokens = 0\n",
    "\n",
    "    for s in phrase:\n",
    "        if '\\n' in s or \\\n",
    "                '\\t' in s or \\\n",
    "                ' ' in s:\n",
    "            tokens += 1\n",
    "\n",
    "    words = phrase.split()\n",
    "\n",
    "    for word in words:\n",
    "        count = syllable_count(word)\n",
    "        tokens += count\n",
    "\n",
    "    if tokens == 0:\n",
    "        tokens = 1\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiou\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "238a93dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ FUNCIONES MODULARES PARA EXIST2025 - VERSIÃ“N OPTIMIZADA\n",
    "\n",
    "class EXIST2025PromptBuilder:\n",
    "    \"\"\"\n",
    "    Clase optimizada para construir prompts de EXIST2025 de forma modular y mantenible\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, language=\"es\"):\n",
    "        self.language = language.lower()\n",
    "        self.definitions = self._load_definitions()\n",
    "        self.examples = self._load_examples()\n",
    "        self.categories = self._load_categories()\n",
    "    \n",
    "    def _load_definitions(self):\n",
    "        \"\"\"Carga las definiciones base segÃºn el idioma\"\"\"\n",
    "        if self.language == \"es\":\n",
    "            return {\n",
    "                \"sexism\": \"Prejuicio, estereotipo o discriminaciÃ³n, tÃ­picamente contra las mujeres, basado en el sexo\",\n",
    "                \"key_question\": \"Â¿Es el tweet sexista de alguna forma, o describe situaciones donde ocurre discriminaciÃ³n?\",\n",
    "                \"intention_question\": \"Â¿CuÃ¡l es la intenciÃ³n de la persona que escribiÃ³ el tweet?\",\n",
    "                \"categories_question\": \"Identifica QUÃ‰ TIPO(S) de sexismo contiene este tweet:\"\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"sexism\": \"Prejudice, stereotyping, or discrimination, typically against women, on the basis of sex\",\n",
    "                \"key_question\": \"Is the tweet sexist in any form, or does it describe situations where discrimination occurs?\",\n",
    "                \"intention_question\": \"What do you think the intention of the person that wrote the tweet is?\",\n",
    "                \"categories_question\": \"Identify WHAT TYPE(S) of sexism this tweet contains:\"\n",
    "            }\n",
    "    \n",
    "    def _load_examples(self):\n",
    "        \"\"\"Carga los ejemplos segÃºn el idioma\"\"\"\n",
    "        if self.language == \"es\":\n",
    "            return {\n",
    "                \"sexist\": [\n",
    "                    \"Es menos un #dÃ­asinmujeres y mÃ¡s un dÃ­a sin feministas, lo cual suena encantador\",\n",
    "                    \"Las mujeres no pueden manejar, llÃ¡mame sexista pero es verdad\",\n",
    "                    \"Te ves como una puta con esos pantalones - Mi hermano cuando me vio\"\n",
    "                ],\n",
    "                \"non_sexist\": [\n",
    "                    \"Â¿DÃ³nde estÃ¡n todas las mujeres blancas?\",\n",
    "                    \"El video impactante de una mujer al volante que escapa milagrosamente\",\n",
    "                    \"Â¿No te convencen mis argumentos? Intentemos debatir\"\n",
    "                ],\n",
    "                \"forms\": {\n",
    "                    \"AMIGABLE\": \"Las mujeres deben ser amadas y respetadas, trÃ¡telas como cristal frÃ¡gil\",\n",
    "                    \"DIVERTIDO\": \"Tienes que amar a las mujeres... solo eso... nunca las entenderÃ¡s\",\n",
    "                    \"OFENSIVO\": \"Te ves como una puta con esos pantalones\",\n",
    "                    \"VIOLENTO\": \"HumÃ­llate y degrÃ¡date como la perra que eres\"\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"sexist\": [\n",
    "                    \"It's less of #adaywithoutwomen and more of a day without feminists, which sounds lovely\",\n",
    "                    \"I'm sorry but women cannot drive, call me sexist but it is true\",\n",
    "                    \"You look like a whore in those pants - My brother when he saw me\"\n",
    "                ],\n",
    "                \"non_sexist\": [\n",
    "                    \"Where all the white women at?\",\n",
    "                    \"The shocking video of a woman at the wheel who miraculously escapes\",\n",
    "                    \"Don't my arguments convince you? Let's try to debate\"\n",
    "                ],\n",
    "                \"forms\": {\n",
    "                    \"FRIENDLY\": \"Women must be loved and respected, always treat them like fragile glass\",\n",
    "                    \"FUNNY\": \"You have to love women... just that... You will never understand them\",\n",
    "                    \"OFFENSIVE\": \"You look like a whore in those pants\",\n",
    "                    \"VIOLENT\": \"Humiliate, expose and degrade yourself as the fucking bitch you are\"\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    def _load_categories(self):\n",
    "        \"\"\"Carga las categorÃ­as de sexismo segÃºn el idioma\"\"\"\n",
    "        if self.language == \"es\":\n",
    "            return {\n",
    "                \"IDEOLOGICAL-INEQUALITY\": {\n",
    "                    \"name\": \"DescrÃ©dito ideolÃ³gico, negaciÃ³n de desigualdad\",\n",
    "                    \"description\": \"Desacredita movimiento feminista para devaluar lucha de mujeres\",\n",
    "                    \"examples\": [\"#El feminismo es una guerra contra los hombres\", \"La igualdad se estÃ¡ saliendo de control\"]\n",
    "                },\n",
    "                \"STEREOTYPING-DOMINANCE\": {\n",
    "                    \"name\": \"Estereotipos de roles y dominancia\",\n",
    "                    \"description\": \"Ideas falsas sobre capacidades de mujeres para ciertas tareas\",\n",
    "                    \"examples\": [\"Las mujeres ya no tienen deseo de desarrollar carÃ¡cter\", \"Mujer al volante, Â¡cuidado!\"]\n",
    "                },\n",
    "                \"OBJECTIFICATION\": {\n",
    "                    \"name\": \"ObjetificaciÃ³n y estereotipos fÃ­sicos\",\n",
    "                    \"description\": \"Presenta mujeres como objetos sin dignidad\",\n",
    "                    \"examples\": [\"Solo quiero mujeres para sexo\", \"Nunca he visto una prostituta atractiva\"]\n",
    "                },\n",
    "                \"SEXUAL-VIOLENCE\": {\n",
    "                    \"name\": \"Violencia sexual\",\n",
    "                    \"description\": \"Sugerencias sexuales, solicitudes de favores sexuales\",\n",
    "                    \"examples\": [\"Quiero tocar tus pechos... no puedes imaginar lo que puedo hacer\"]\n",
    "                },\n",
    "                \"MISOGYNY-NON-SEXUAL-VIOLENCE\": {\n",
    "                    \"name\": \"Misoginia y violencia no sexual\",\n",
    "                    \"description\": \"Expresiones de odio y violencia hacia mujeres\",\n",
    "                    \"examples\": [\"Algunas mujeres son tan tÃ³xicas\", \"El abuso domÃ©stico... a menos que tu esposa sea una perra\"]\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"IDEOLOGICAL-INEQUALITY\": {\n",
    "                    \"name\": \"Ideological discredit, denying inequality\",\n",
    "                    \"description\": \"Discredits feminist movement to devalue women's struggle\",\n",
    "                    \"examples\": [\"#Feminism is a war on men\", \"The whole equality thing is getting out of hand\"]\n",
    "                },\n",
    "                \"STEREOTYPING-DOMINANCE\": {\n",
    "                    \"name\": \"Role stereotyping and dominance\",\n",
    "                    \"description\": \"False ideas about women's capabilities for certain tasks\",\n",
    "                    \"examples\": [\"Most women no longer have the desire to develop character\", \"Woman driving, be careful!\"]\n",
    "                },\n",
    "                \"OBJECTIFICATION\": {\n",
    "                    \"name\": \"Objectification and physical stereotyping\",\n",
    "                    \"description\": \"Presents women as objects without dignity\",\n",
    "                    \"examples\": [\"I just want women for sex\", \"I've never seen an attractive hooker\"]\n",
    "                },\n",
    "                \"SEXUAL-VIOLENCE\": {\n",
    "                    \"name\": \"Sexual violence\",\n",
    "                    \"description\": \"Sexual suggestions, requests for sexual favors\",\n",
    "                    \"examples\": [\"I wanna touch your tits...you can't imagine what I can do\"]\n",
    "                },\n",
    "                \"MISOGYNY-NON-SEXUAL-VIOLENCE\": {\n",
    "                    \"name\": \"Misogyny and non-sexual violence\",\n",
    "                    \"description\": \"Expressions of hatred and violence towards women\",\n",
    "                    \"examples\": [\"Some women are so toxic\", \"Domestic abuse... Unless your wife is a bitch\"]\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    def build_header(self, subtask, tweet):\n",
    "        \"\"\"Construye el header del prompt\"\"\"\n",
    "        task_names = {\n",
    "            \"1.1\": \"DetecciÃ³n Binaria de Sexismo\" if self.language == \"es\" else \"Binary Sexism Detection\",\n",
    "            \"1.2\": \"IntenciÃ³n del Emisor\" if self.language == \"es\" else \"Author's Intention\",\n",
    "            \"1.3\": \"CategorÃ­as de Sexismo (Multilabel)\" if self.language == \"es\" else \"Sexism Categories (Multilabel)\"\n",
    "        }\n",
    "        \n",
    "        return f'''EXIST2025 - Subtask {subtask}: {task_names[subtask]}\n",
    "\n",
    "{\"Analiza el siguiente tweet segÃºn los criterios oficiales de EXIST2025:\" if self.language == \"es\" else \"Analyze the following tweet according to official EXIST2025 criteria:\"}\n",
    "\n",
    "TWEET: \"{tweet}\"\n",
    "'''\n",
    "    \n",
    "    def build_binary_prompt(self, tweet):\n",
    "        \"\"\"Construye prompt para subtask 1.1\"\"\"\n",
    "        header = self.build_header(\"1.1\", tweet)\n",
    "        \n",
    "        if self.language == \"es\":\n",
    "            content = f'''\n",
    "PREGUNTA CLAVE: {self.definitions[\"key_question\"]}\n",
    "\n",
    "DEFINICIÃ“N SEXISMO (Oxford Dictionary + EXIST2025):\n",
    "\"{self.definitions[\"sexism\"]}\"\n",
    "\n",
    "INCLUYE (Clasifica como YES):\n",
    "â€¢ Tweet sexista por sÃ­ mismo\n",
    "â€¢ Tweet que describe una situaciÃ³n sexista\n",
    "â€¢ Tweet que critica un comportamiento sexista\n",
    "\n",
    "FORMAS DE SEXISMO A DETECTAR:\n",
    "ðŸ”¹ AMIGABLE: \"{self.examples[\"forms\"][\"AMIGABLE\"]}\"\n",
    "ðŸ”¹ DIVERTIDO: \"{self.examples[\"forms\"][\"DIVERTIDO\"]}\"\n",
    "ðŸ”¹ OFENSIVO: \"{self.examples[\"forms\"][\"OFENSIVO\"]}\"\n",
    "ðŸ”¹ VIOLENTO: \"{self.examples[\"forms\"][\"VIOLENTO\"]}\"\n",
    "\n",
    "EJEMPLOS SEXISTAS (YES):\n",
    "{chr(10).join([f\"- {ex}\" for ex in self.examples[\"sexist\"]])}\n",
    "\n",
    "EJEMPLOS NO SEXISTAS (NO):\n",
    "{chr(10).join([f\"- {ex}\" for ex in self.examples[\"non_sexist\"]])}\n",
    "\n",
    "RESPUESTA: YES o NO\n",
    "Responde ÃšNICAMENTE: YES o NO\n",
    "'''\n",
    "        else:\n",
    "            content = f'''\n",
    "KEY QUESTION: {self.definitions[\"key_question\"]}\n",
    "\n",
    "SEXISM DEFINITION (Oxford Dictionary + EXIST2025):\n",
    "\"{self.definitions[\"sexism\"]}\"\n",
    "\n",
    "INCLUDES (Classify as YES):\n",
    "â€¢ Tweet that is sexist itself\n",
    "â€¢ Tweet that describes a sexist situation\n",
    "â€¢ Tweet that criticizes sexist behavior\n",
    "\n",
    "FORMS OF SEXISM TO DETECT:\n",
    "ðŸ”¹ FRIENDLY: \"{self.examples[\"forms\"][\"FRIENDLY\"]}\"\n",
    "ðŸ”¹ FUNNY: \"{self.examples[\"forms\"][\"FUNNY\"]}\"\n",
    "ðŸ”¹ OFFENSIVE: \"{self.examples[\"forms\"][\"OFFENSIVE\"]}\"\n",
    "ðŸ”¹ VIOLENT: \"{self.examples[\"forms\"][\"VIOLENT\"]}\"\n",
    "\n",
    "SEXIST EXAMPLES (YES):\n",
    "{chr(10).join([f\"- {ex}\" for ex in self.examples[\"sexist\"]])}\n",
    "\n",
    "NON-SEXIST EXAMPLES (NO):\n",
    "{chr(10).join([f\"- {ex}\" for ex in self.examples[\"non_sexist\"]])}\n",
    "\n",
    "RESPONSE: YES or NO\n",
    "Respond ONLY: YES or NO\n",
    "'''\n",
    "        \n",
    "        return header + content\n",
    "    \n",
    "    def build_intention_prompt(self, tweet):\n",
    "        \"\"\"Construye prompt para subtask 1.2\"\"\"\n",
    "        header = self.build_header(\"1.2\", tweet)\n",
    "        \n",
    "        intentions = {\n",
    "            \"es\": {\n",
    "                \"DIRECT\": {\n",
    "                    \"desc\": \"La intenciÃ³n es escribir un mensaje sexista por sÃ­ mismo\",\n",
    "                    \"example\": \"Una mujer necesita amor, llenar la nevera, si un hombre puede darle esto a cambio de sus servicios (tareas domÃ©sticas, cocina, etc), no veo quÃ© mÃ¡s necesita\"\n",
    "                },\n",
    "                \"REPORTED\": {\n",
    "                    \"desc\": \"La intenciÃ³n es reportar o describir una situaciÃ³n sexista sufrida por una mujer (primera o tercera persona)\",\n",
    "                    \"example\": \"Me duermo en el metro, abro los ojos sintiendo algo raro: la mano del hombre sentado a mi lado en mi pierna #DenunciaATuCerdo\"\n",
    "                },\n",
    "                \"JUDGEMENTAL\": {\n",
    "                    \"desc\": \"La intenciÃ³n es ser crÃ­tico, describiendo situaciones sexistas con el objetivo de condenarlas\",\n",
    "                    \"example\": \"Como siempre, la mujer fue quien dejÃ³ su trabajo por el bienestar de la familia\"\n",
    "                }\n",
    "            },\n",
    "            \"en\": {\n",
    "                \"DIRECT\": {\n",
    "                    \"desc\": \"The intention is to write a message that is sexist by itself\",\n",
    "                    \"example\": \"A woman needs love, to fill the fridge, if a man can give this to her in return for her services (housework, cooking, etc), I don't see what else she needs\"\n",
    "                },\n",
    "                \"REPORTED\": {\n",
    "                    \"desc\": \"The intention is to report or describe a sexist situation or event suffered by a woman (first or third person)\",\n",
    "                    \"example\": \"I doze in the subway, I open my eyes feeling something weird: the hand of the man sat next to me on my leg #SquealOnYourPig\"\n",
    "                },\n",
    "                \"JUDGEMENTAL\": {\n",
    "                    \"desc\": \"The intention is to be judgemental, describing sexist situations or behaviors with the aim to condemning them\",\n",
    "                    \"example\": \"As usual, the woman was the one quitting her job for the family's welfare\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        lang_intentions = intentions[self.language]\n",
    "        \n",
    "        if self.language == \"es\":\n",
    "            content = f'''\n",
    "PREGUNTA: {self.definitions[\"intention_question\"]}\n",
    "\n",
    "OPCIONES DE INTENCIÃ“N:\n",
    "\n",
    "ðŸ”¹ DIRECT: {lang_intentions[\"DIRECT\"][\"desc\"]}\n",
    "Ejemplo: \"{lang_intentions[\"DIRECT\"][\"example\"]}\"\n",
    "\n",
    "ðŸ”¹ REPORTED: {lang_intentions[\"REPORTED\"][\"desc\"]}\n",
    "Ejemplo: \"{lang_intentions[\"REPORTED\"][\"example\"]}\"\n",
    "\n",
    "ðŸ”¹ JUDGEMENTAL: {lang_intentions[\"JUDGEMENTAL\"][\"desc\"]}\n",
    "Ejemplo: \"{lang_intentions[\"JUDGEMENTAL\"][\"example\"]}\"\n",
    "\n",
    "RESPUESTA: DIRECT, REPORTED o JUDGEMENTAL\n",
    "Responde ÃšNICAMENTE: DIRECT, REPORTED o JUDGEMENTAL\n",
    "'''\n",
    "        else:\n",
    "            content = f'''\n",
    "QUESTION: {self.definitions[\"intention_question\"]}\n",
    "\n",
    "INTENTION OPTIONS:\n",
    "\n",
    "ðŸ”¹ DIRECT: {lang_intentions[\"DIRECT\"][\"desc\"]}\n",
    "Example: \"{lang_intentions[\"DIRECT\"][\"example\"]}\"\n",
    "\n",
    "ðŸ”¹ REPORTED: {lang_intentions[\"REPORTED\"][\"desc\"]}\n",
    "Example: \"{lang_intentions[\"REPORTED\"][\"example\"]}\"\n",
    "\n",
    "ðŸ”¹ JUDGEMENTAL: {lang_intentions[\"JUDGEMENTAL\"][\"desc\"]}\n",
    "Example: \"{lang_intentions[\"JUDGEMENTAL\"][\"example\"]}\"\n",
    "\n",
    "RESPONSE: DIRECT, REPORTED or JUDGEMENTAL\n",
    "Respond ONLY: DIRECT, REPORTED or JUDGEMENTAL\n",
    "'''\n",
    "        \n",
    "        return header + content\n",
    "    \n",
    "    def build_categories_prompt(self, tweet):\n",
    "        \"\"\"Construye prompt para subtask 1.3\"\"\"\n",
    "        header = self.build_header(\"1.3\", tweet)\n",
    "        \n",
    "        if self.language == \"es\":\n",
    "            content = f'''\n",
    "{self.definitions[\"categories_question\"]}\n",
    "\n",
    "CATEGORÃAS POSIBLES (puede ser una o varias):\n",
    "\n",
    "'''\n",
    "            for cat_id, cat_info in self.categories.items():\n",
    "                content += f'''ðŸ”¹ {cat_id}: {cat_info[\"name\"]}\n",
    "- {cat_info[\"description\"]}\n",
    "Ejemplos: {\" / \".join(cat_info[\"examples\"])}\n",
    "\n",
    "'''\n",
    "            \n",
    "            content += '''RESPUESTA: Lista las categorÃ­as aplicables separadas por comas\n",
    "Responde con las categorÃ­as que aplican, ejemplo: IDEOLOGICAL-INEQUALITY, STEREOTYPING-DOMINANCE\n",
    "'''\n",
    "        else:\n",
    "            content = f'''\n",
    "{self.definitions[\"categories_question\"]}\n",
    "\n",
    "POSSIBLE CATEGORIES (can be one or several):\n",
    "\n",
    "'''\n",
    "            for cat_id, cat_info in self.categories.items():\n",
    "                content += f'''ðŸ”¹ {cat_id}: {cat_info[\"name\"]}\n",
    "- {cat_info[\"description\"]}\n",
    "Examples: {\" / \".join(cat_info[\"examples\"])}\n",
    "\n",
    "'''\n",
    "            \n",
    "            content += '''RESPONSE: List applicable categories separated by commas\n",
    "Respond with the categories that apply, example: IDEOLOGICAL-INEQUALITY, STEREOTYPING-DOMINANCE\n",
    "'''\n",
    "        \n",
    "        return header + content\n",
    "    \n",
    "    def build_prompt(self, tweet, subtask=\"1.1\"):\n",
    "        \"\"\"MÃ©todo principal para construir prompts\"\"\"\n",
    "        if subtask == \"1.1\":\n",
    "            return self.build_binary_prompt(tweet)\n",
    "        elif subtask == \"1.2\":\n",
    "            return self.build_intention_prompt(tweet)\n",
    "        elif subtask == \"1.3\":\n",
    "            return self.build_categories_prompt(tweet)\n",
    "        else:\n",
    "            raise ValueError(f\"Subtask '{subtask}' no vÃ¡lida. Use '1.1', '1.2', o '1.3'\")\n",
    "\n",
    "\n",
    "# ðŸš€ FUNCIÃ“N OPTIMIZADA CON LA NUEVA CLASE\n",
    "def build_prompt_sexism_detection(post, subtask=\"1.1\", language=\"es\"):\n",
    "    \"\"\"\n",
    "    FunciÃ³n optimizada para construir prompts de EXIST2025\n",
    "    \n",
    "    Args:\n",
    "        post (str): El texto del tweet a analizar\n",
    "        subtask (str): \"1.1\" (binario), \"1.2\" (intenciÃ³n), \"1.3\" (categorÃ­as)\n",
    "        language (str): El idioma del anÃ¡lisis (\"es\" para espaÃ±ol, \"en\" para inglÃ©s)\n",
    "    \n",
    "    Returns:\n",
    "        str: Prompt formateado segÃºn la subtarea especÃ­fica\n",
    "    \"\"\"\n",
    "    builder = EXIST2025PromptBuilder(language)\n",
    "    return builder.build_prompt(post, subtask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f0d51c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#como usarlo\n",
    "# Ejemplo de uso:       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "541a7935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXIST2025 - Subtask 1.1: DetecciÃ³n Binaria de Sexismo\n",
      "\n",
      "Analiza el siguiente tweet segÃºn los criterios oficiales de EXIST2025:\n",
      "\n",
      "TWEET: \"Las mujeres no pueden manejar, llÃ¡mame sexista pero es verdad\"\n",
      "\n",
      "PREGUNTA CLAVE: Â¿Es el tweet sexista de alguna forma, o describe situaciones donde ocurre discriminaciÃ³n?\n",
      "\n",
      "DEFINICIÃ“N SEXISMO (Oxford Dictionary + EXIST2025):\n",
      "\"Prejuicio, estereotipo o discriminaciÃ³n, tÃ­picamente contra las mujeres, basado en el sexo\"\n",
      "\n",
      "INCLUYE (Clasifica como YES):\n",
      "â€¢ Tweet sexista por sÃ­ mismo\n",
      "â€¢ Tweet que describe una situaciÃ³n sexista\n",
      "â€¢ Tweet que critica un comportamiento sexista\n",
      "\n",
      "FORMAS DE SEXISMO A DETECTAR:\n",
      "ðŸ”¹ AMIGABLE: \"Las mujeres deben ser amadas y respetadas, trÃ¡telas como cristal frÃ¡gil\"\n",
      "ðŸ”¹ DIVERTIDO: \"Tienes que amar a las mujeres... solo eso... nunca las entenderÃ¡s\"\n",
      "ðŸ”¹ OFENSIVO: \"Te ves como una puta con esos pantalones\"\n",
      "ðŸ”¹ VIOLENTO: \"HumÃ­llate y degrÃ¡date como la perra que eres\"\n",
      "\n",
      "EJEMPLOS SEXISTAS (YES):\n",
      "- Es menos un #dÃ­asinmujeres y mÃ¡s un dÃ­a sin feministas, lo cual suena encantador\n",
      "- Las mujeres no pueden manejar, llÃ¡mame sexista pero es verdad\n",
      "- Te ves como una puta con esos pantalones - Mi hermano cuando me vio\n",
      "\n",
      "EJEMPLOS NO SEXISTAS (NO):\n",
      "- Â¿DÃ³nde estÃ¡n todas las mujeres blancas?\n",
      "- El video impactante de una mujer al volante que escapa milagrosamente\n",
      "- Â¿No te convencen mis argumentos? Intentemos debatir\n",
      "\n",
      "RESPUESTA: YES o NO\n",
      "Responde ÃšNICAMENTE: YES o NO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso de la clase EXIST2025PromptBuilder y la funciÃ³n build_prompt_sexism_detection\n",
    "\n",
    "# Para construir un prompt para la subtarea 1.1 (detecciÃ³n binaria de sexismo) en espaÃ±ol:\n",
    "tweet_ejemplo = \"Las mujeres no pueden manejar, llÃ¡mame sexista pero es verdad\"\n",
    "prompt = build_prompt_sexism_detection(tweet_ejemplo, subtask=\"1.1\", language=\"es\")\n",
    "print(prompt)\n",
    "\n",
    "# Para inglÃ©s:\n",
    "# = build_prompt_sexism_detection(tweet_ejemplo, subtask=\"1.1\", language=\"en\")\n",
    "#print(prompt_en)\n",
    "\n",
    "# TambiÃ©n puedes usar directamente el builder:\n",
    "#prompt_1_3 = builder_es.build_prompt(tweet_ejemplo, subtask=\"1.3\")\n",
    "#print(prompt_1_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b313ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama al modelo decoupled \n",
    "respuesta, modelo_usado, tokens_in, tokens_out = prompt_decoupled(prompt, model='mistral-nemo:latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "928e7159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTADOS:\n",
      "Modelo usado: mistral-nemo:latest\n",
      "Tokens de entrada: 678\n",
      "Tokens de salida: 1\n",
      "Respuesta: YES\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nðŸ“Š RESULTADOS:\")\n",
    "print(f\"Modelo usado: {modelo_usado}\")\n",
    "print(f\"Tokens de entrada: {tokens_in}\")\n",
    "print(f\"Tokens de salida: {tokens_out}\")\n",
    "print(f\"Respuesta: {respuesta['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d352f221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1.2 enviado:\n",
      "EXIST2025 - Subtask 1.2: IntenciÃ³n del Emisor\n",
      "\n",
      "Analiza el siguiente tweet segÃºn los criterios oficiales de EXIST2025:\n",
      "\n",
      "TWEET: \"Las mujeres no pueden manejar, llÃ¡mame sexista pero es verdad\"\n",
      "\n",
      "PREGUNTA: Â¿CuÃ¡l es la intenciÃ³n de la persona que escribiÃ³ el tweet?\n",
      "\n",
      "OPCIONES DE INTENCIÃ“N:\n",
      "\n",
      "ðŸ”¹ DIRECT: La intenciÃ³n es escribir un mensaje sexista por sÃ­ mismo\n",
      "Ejemplo: \"Una mujer necesita amor, llenar la nevera, si un hombre puede darle esto a cambio de sus servicios (tareas domÃ©sticas, cocina, etc), no veo quÃ© mÃ¡s necesita\"\n",
      "\n",
      "ðŸ”¹ REPORTED: La intenciÃ³n es reportar o describir una situaciÃ³n sexista sufrida por una mujer (primera o tercera persona)\n",
      "Ejemplo: \"Me duermo en el metro, abro los ojos sintiendo algo raro: la mano del hombre sentado a mi lado en mi pierna #DenunciaATuCerdo\"\n",
      "\n",
      "ðŸ”¹ JUDGEMENTAL: La intenciÃ³n es ser crÃ­tico, describiendo situaciones sexistas con el objetivo de condenarlas\n",
      "Ejemplo: \"Como siempre, la mujer fue quien dejÃ³ su trabajo por el bienestar de la familia\"\n",
      "\n",
      "RESPUESTA: DIRECT, REPORTED o JUDGEMENTAL\n",
      "Responde ÃšNICAMENTE: DIRECT, REPORTED o JUDGEMENTAL\n",
      "\n",
      "\n",
      "Respuesta 1.2:\n",
      "DIRECT\n"
     ]
    }
   ],
   "source": [
    "# Probar las otras dos subtareas (1.2 y 1.3) con el tweet de ejemplo\n",
    "\n",
    "# Subtarea 1.2: IntenciÃ³n del emisor\n",
    "prompt_1_2 = build_prompt_sexism_detection(tweet_ejemplo, subtask=\"1.2\", language=\"es\")\n",
    "respuesta_1_2, modelo_usado_1_2, tokens_in_1_2, tokens_out_1_2 = prompt_decoupled(prompt_1_2, model='mistral-nemo:latest')\n",
    "\n",
    "print(\"Prompt 1.2 enviado:\")\n",
    "print(prompt_1_2)\n",
    "print(f\"\\nRespuesta 1.2:\\n{respuesta_1_2['response']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f393f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 1.3 enviado:\n",
      "EXIST2025 - Subtask 1.3: CategorÃ­as de Sexismo (Multilabel)\n",
      "\n",
      "Analiza el siguiente tweet segÃºn los criterios oficiales de EXIST2025:\n",
      "\n",
      "TWEET: \"Las mujeres no pueden manejar, llÃ¡mame sexista pero es verdad\"\n",
      "\n",
      "Identifica QUÃ‰ TIPO(S) de sexismo contiene este tweet:\n",
      "\n",
      "CATEGORÃAS POSIBLES (puede ser una o varias):\n",
      "\n",
      "ðŸ”¹ IDEOLOGICAL-INEQUALITY: DescrÃ©dito ideolÃ³gico, negaciÃ³n de desigualdad\n",
      "- Desacredita movimiento feminista para devaluar lucha de mujeres\n",
      "Ejemplos: #El feminismo es una guerra contra los hombres / La igualdad se estÃ¡ saliendo de control\n",
      "\n",
      "ðŸ”¹ STEREOTYPING-DOMINANCE: Estereotipos de roles y dominancia\n",
      "- Ideas falsas sobre capacidades de mujeres para ciertas tareas\n",
      "Ejemplos: Las mujeres ya no tienen deseo de desarrollar carÃ¡cter / Mujer al volante, Â¡cuidado!\n",
      "\n",
      "ðŸ”¹ OBJECTIFICATION: ObjetificaciÃ³n y estereotipos fÃ­sicos\n",
      "- Presenta mujeres como objetos sin dignidad\n",
      "Ejemplos: Solo quiero mujeres para sexo / Nunca he visto una prostituta atractiva\n",
      "\n",
      "ðŸ”¹ SEXUAL-VIOLENCE: Violencia sexual\n",
      "- Sugerencias sexuales, solicitudes de favores sexuales\n",
      "Ejemplos: Quiero tocar tus pechos... no puedes imaginar lo que puedo hacer\n",
      "\n",
      "ðŸ”¹ MISOGYNY-NON-SEXUAL-VIOLENCE: Misoginia y violencia no sexual\n",
      "- Expresiones de odio y violencia hacia mujeres\n",
      "Ejemplos: Algunas mujeres son tan tÃ³xicas / El abuso domÃ©stico... a menos que tu esposa sea una perra\n",
      "\n",
      "RESPUESTA: Lista las categorÃ­as aplicables separadas por comas\n",
      "Responde con las categorÃ­as que aplican, ejemplo: IDEOLOGICAL-INEQUALITY, STEREOTYPING-DOMINANCE\n",
      "\n",
      "\n",
      "Respuesta 1.3:\n",
      "El tweet \"Las mujeres no pueden manejar, llÃ¡mame sexista pero es verdad\" contiene los siguientes tipos de sexismo:\n",
      "\n",
      "STEREOTYPING-DOMINANCE\n"
     ]
    }
   ],
   "source": [
    "# Subtarea 1.3: CategorÃ­as de sexismo\n",
    "prompt_1_3 = build_prompt_sexism_detection(tweet_ejemplo, subtask=\"1.3\", language=\"es\")\n",
    "respuesta_1_3, modelo_usado_1_3, tokens_in_1_3, tokens_out_1_3 = prompt_decoupled(prompt_1_3, model='mistral-nemo:latest')\n",
    "\n",
    "print(\"\\nPrompt 1.3 enviado:\")\n",
    "print(prompt_1_3)\n",
    "print(f\"\\nRespuesta 1.3:\\n{respuesta_1_3['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38fb0ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dispositivo seleccionado: cuda\n",
      "ðŸ”¥ Nombre de la GPU: NVIDIA GeForce RTX 4080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”§ SISTEMA HÃBRIDO: TRANSFORMER + LLM ENSEMBLE\n",
    "\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig, Trainer,AutoModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "# Detectar si hay una GPU disponible (CUDA) y seleccionarla, de lo contrario, usar CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"âœ… Dispositivo seleccionado: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ðŸ”¥ Nombre de la GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e61143a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Clase SoftBinaryClassifier definida correctamente.\n"
     ]
    }
   ],
   "source": [
    "# CELDA [38] CORREGIDA\n",
    "\n",
    "class SoftBinaryClassifier(nn.Module):\n",
    "    MODEL_NAME = \"xlm-roberta-large\" #microsoft/mdeberta-v3-base\"\n",
    "    def __init__(self, model_name: str =MODEL_NAME, dropout: float = 0.3):\n",
    "        \"\"\"\n",
    "        Inicializa la arquitectura del modelo a partir de un nombre de modelo base.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Cargar la configuraciÃ³n y el encoder del modelo base\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Capas personalizadas para la clasificaciÃ³n\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, 1)\n",
    "        self.to(device)\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Pase hacia adelante (forward pass). Acepta **kwargs para ignorar argumentos extra.\n",
    "        \"\"\"\n",
    "        output = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = self.dropout(output.last_hidden_state[:, 0])\n",
    "        logits = self.classifier(pooled).squeeze(-1)\n",
    "        return SequenceClassifierOutput(loss=None, logits=logits)\n",
    "\n",
    "print(\"âœ… Clase SoftBinaryClassifier definida correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2011f3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Clase EXIST2025HybridSystem definida correctamente\n",
      "ðŸš€ Listo para crear instancia del sistema hÃ­brido\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class EXIST2025HybridSystem:\n",
    "    \"\"\"\n",
    "    Sistema hÃ­brido que combina Transformer local + LLM remoto para EXIST2025\n",
    "    \"\"\"\n",
    "    def __init__(self, transformer_model_path: str = None):\n",
    "        \"\"\"\n",
    "        Inicializador del sistema. Recibe la ruta al modelo afinado.\n",
    "        \"\"\"\n",
    "        self.transformer_model = None\n",
    "        self.transformer_tokenizer = None\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.llm_builder = EXIST2025PromptBuilder(language=\"es\")\n",
    "\n",
    "        # ConfiguraciÃ³n del ensemble\n",
    "        self.weights = {\n",
    "            'transformer': 0.6,  # Peso del transformer (mÃ¡s especializado)\n",
    "            'llm': 0.4           # Peso del LLM (mÃ¡s generalista)\n",
    "        }\n",
    "        \n",
    "        # Thresholds optimizados\n",
    "        self.thresholds = {\n",
    "            'confidence_min': 0.7,  # Confianza mÃ­nima para decisiÃ³n unilateral\n",
    "            'agreement_threshold': 0.8  # Umbral para considerar acuerdo entre modelos\n",
    "        }\n",
    "        \n",
    "        if transformer_model_path:\n",
    "            self.load_finetuned_transformer(transformer_model_path)\n",
    "\n",
    "    def load_finetuned_transformer(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Carga el modelo transformer afinado de forma correcta.\n",
    "        \"\"\"\n",
    "        print(f\"ðŸ”§ Cargando modelo afinado desde: {model_path}\")\n",
    "        try:\n",
    "            from safetensors.torch import load_file\n",
    "            # Cargar tokenizer\n",
    "            self.transformer_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            print(\"   âœ… Tokenizer cargado.\")\n",
    "\n",
    "            # Cargar config para saber quÃ© modelo base se usÃ³ (ej: 'xlm-roberta-large')\n",
    "            #config = AutoConfig.from_pretrained(model_path)\n",
    "            #base_model_name = config._name_or_path\n",
    "            #print(f\"   âœ… ConfiguraciÃ³n leÃ­da. Modelo base: {base_model_name}\")\n",
    "\n",
    "            # Crear la \"carcasa\" del modelo usando el nombre del modelo base\n",
    "            self.transformer_model = SoftBinaryClassifier()\n",
    "            print(\"   âœ… Arquitectura SoftBinaryClassifier instanciada.\")\n",
    "\n",
    "            # Cargar los pesos entrenados en la carcasa\n",
    "            weights_path = os.path.join(model_path, 'model.safetensors')\n",
    "            state_dict = load_file(weights_path, device=str(self.device))\n",
    "            self.transformer_model.load_state_dict(state_dict)\n",
    "            print(\"   âœ… Pesos (safetensors) cargados en el modelo.\")\n",
    "\n",
    "            # Mover a GPU y poner en modo evaluaciÃ³n\n",
    "            self.transformer_model.to(self.device)\n",
    "            self.transformer_model.eval()\n",
    "            print(f\"   ðŸ’» Modelo listo en dispositivo: {self.transformer_model.device}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error al cargar modelo: {e}\")\n",
    "            self.transformer_model = None\n",
    "            self.transformer_tokenizer = None\n",
    "\n",
    "    def predict_transformer(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Realiza una predicciÃ³n usando el modelo Transformer cargado.\n",
    "        \"\"\"\n",
    "        if self.transformer_model is None or self.transformer_tokenizer is None:\n",
    "            return {'available': False, 'probability': 0.5}\n",
    "        try:\n",
    "            # Tokenizar y mover a GPU\n",
    "            inputs = self.transformer_tokenizer(text, return_tensors=\"pt\")\n",
    "            inputs = {key: val.to(self.device) for key, val in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.transformer_model(**inputs)\n",
    "                prob = torch.sigmoid(outputs.logits).item()\n",
    "            \n",
    "            prediction = \"YES\" if prob > 0.5 else \"NO\"\n",
    "            confidence = abs(prob - 0.5) * 2\n",
    "            \n",
    "            return {\n",
    "                'available': True, 'prediction': prediction,\n",
    "                'probability': prob, 'confidence': confidence\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error en predicciÃ³n Transformer: {e}\")\n",
    "            return {'available': False, 'probability': 0.5}\n",
    "        \n",
    "        \n",
    "    def predict_llm(self, text: str, subtask: str = \"1.1\") -> Dict:\n",
    "        \"\"\"\n",
    "        PredicciÃ³n usando LLM remoto\n",
    "        \n",
    "        Args:\n",
    "            text: Texto a analizar\n",
    "            subtask: Subtarea EXIST2025 (\"1.1\", \"1.2\", \"1.3\")\n",
    "            \n",
    "        Returns:\n",
    "            Dict con predicciÃ³n y metadatos\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Construir prompt\n",
    "            prompt = self.llm_builder.build_prompt(text, subtask)\n",
    "            \n",
    "            # Llamar a LLM\n",
    "            response, model_used, tokens_in, tokens_out = prompt_decoupled(\n",
    "                prompt, \n",
    "                model='mistral-nemo:latest'\n",
    "            )\n",
    "            \n",
    "            if response and 'response' in response:\n",
    "                raw_response = response['response'].strip()\n",
    "                \n",
    "                # Procesar respuesta segÃºn subtarea\n",
    "                if subtask == \"1.1\":\n",
    "                    # Extraer YES/NO\n",
    "                    prediction = \"YES\" if \"YES\" in raw_response.upper() else \"NO\"\n",
    "                    confidence = 0.8  # EstimaciÃ³n de confianza para LLM\n",
    "                    \n",
    "                    return {\n",
    "                        'available': True,\n",
    "                        'prediction': prediction,\n",
    "                        'confidence': confidence,\n",
    "                        'raw_response': raw_response,\n",
    "                        'tokens_used': tokens_in + tokens_out,\n",
    "                        'model': model_used\n",
    "                    }\n",
    "                \n",
    "                elif subtask == \"1.2\":\n",
    "                    # Extraer intenciÃ³n\n",
    "                    for intention in [\"DIRECT\", \"REPORTED\", \"JUDGEMENTAL\"]:\n",
    "                        if intention in raw_response.upper():\n",
    "                            return {\n",
    "                                'available': True,\n",
    "                                'prediction': intention,\n",
    "                                'confidence': 0.8,\n",
    "                                'raw_response': raw_response,\n",
    "                                'tokens_used': tokens_in + tokens_out,\n",
    "                                'model': model_used\n",
    "                            }\n",
    "                \n",
    "                elif subtask == \"1.3\":\n",
    "                    # Extraer categorÃ­as\n",
    "                    categories = []\n",
    "                    for cat in [\"IDEOLOGICAL-INEQUALITY\", \"STEREOTYPING-DOMINANCE\", \n",
    "                               \"OBJECTIFICATION\", \"SEXUAL-VIOLENCE\", \"MISOGYNY-NON-SEXUAL-VIOLENCE\"]:\n",
    "                        if cat in raw_response.upper():\n",
    "                            categories.append(cat)\n",
    "                    \n",
    "                    return {\n",
    "                        'available': True,\n",
    "                        'prediction': categories,\n",
    "                        'confidence': 0.8,\n",
    "                        'raw_response': raw_response,\n",
    "                        'tokens_used': tokens_in + tokens_out,\n",
    "                        'model': model_used\n",
    "                    }\n",
    "            \n",
    "            return {\n",
    "                'available': False,\n",
    "                'prediction': None,\n",
    "                'confidence': 0.0,\n",
    "                'error': 'No response from LLM'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error en predicciÃ³n LLM: {e}\")\n",
    "            return {\n",
    "                'available': False,\n",
    "                'prediction': None,\n",
    "                'confidence': 0.0,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def ensemble_prediction(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        PredicciÃ³n ensemble combinando Transformer + LLM\n",
    "        \n",
    "        Args:\n",
    "            text: Texto a analizar\n",
    "            \n",
    "        Returns:\n",
    "            Dict con predicciÃ³n consolidada y anÃ¡lisis completo\n",
    "        \"\"\"\n",
    "        print(f\"ðŸ” Analizando: {text[:50]}...\")\n",
    "        \n",
    "        # Obtener predicciones de ambos modelos\n",
    "        transformer_result = self.predict_transformer(text)\n",
    "        llm_result = self.predict_llm(text, \"1.1\")\n",
    "        \n",
    "        # Predicciones adicionales del LLM\n",
    "        llm_intention = self.predict_llm(text, \"1.2\")\n",
    "        llm_categories = self.predict_llm(text, \"1.3\")\n",
    "        \n",
    "        # Estrategia de ensemble para subtask 1.1\n",
    "        final_prediction = self._ensemble_binary_decision(transformer_result, llm_result)\n",
    "        \n",
    "        # Compilar resultado completo\n",
    "        result = {\n",
    "            'text': text,\n",
    "            'subtask_1_1': final_prediction,\n",
    "            'subtask_1_2': llm_intention,\n",
    "            'subtask_1_3': llm_categories,\n",
    "            'models_used': {\n",
    "                'transformer': transformer_result.get('available', False),\n",
    "                'llm': llm_result.get('available', False)\n",
    "            },\n",
    "            'ensemble_strategy': self._get_strategy_used(transformer_result, llm_result)\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _ensemble_binary_decision(self, transformer_result: Dict, llm_result: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        LÃ³gica de ensemble para decisiÃ³n binaria\n",
    "        \"\"\"\n",
    "        # Caso 1: Solo Transformer disponible\n",
    "        if transformer_result['available'] and not llm_result['available']:\n",
    "            return {\n",
    "                'prediction': transformer_result['prediction'],\n",
    "                'confidence': transformer_result['confidence'],\n",
    "                'method': 'transformer_only',\n",
    "                'reasoning': 'Solo Transformer disponible'\n",
    "            }\n",
    "        \n",
    "        # Caso 2: Solo LLM disponible\n",
    "        if llm_result['available'] and not transformer_result['available']:\n",
    "            return {\n",
    "                'prediction': llm_result['prediction'],\n",
    "                'confidence': llm_result['confidence'],\n",
    "                'method': 'llm_only',\n",
    "                'reasoning': 'Solo LLM disponible'\n",
    "            }\n",
    "        \n",
    "        # Caso 3: Ambos modelos disponibles\n",
    "        if transformer_result['available'] and llm_result['available']:\n",
    "            t_pred = transformer_result['prediction']\n",
    "            t_conf = transformer_result['confidence']\n",
    "            l_pred = llm_result['prediction']\n",
    "            l_conf = llm_result['confidence']\n",
    "            \n",
    "            # Acuerdo entre modelos\n",
    "            if t_pred == l_pred:\n",
    "                avg_confidence = (t_conf * self.weights['transformer'] + \n",
    "                                l_conf * self.weights['llm'])\n",
    "                return {\n",
    "                    'prediction': t_pred,\n",
    "                    'confidence': min(avg_confidence, 0.95),  # Cap mÃ¡ximo\n",
    "                    'method': 'agreement',\n",
    "                    'reasoning': f'Ambos modelos acordaron: {t_pred}'\n",
    "                }\n",
    "            \n",
    "            # Desacuerdo - usar confianza y pesos\n",
    "            if t_conf >= self.thresholds['confidence_min']:\n",
    "                return {\n",
    "                    'prediction': t_pred,\n",
    "                    'confidence': t_conf * 0.9,  # Penalizar por desacuerdo\n",
    "                    'method': 'transformer_confident',\n",
    "                    'reasoning': f'Transformer mÃ¡s confiado: {t_conf:.2f} vs {l_conf:.2f}'\n",
    "                }\n",
    "            \n",
    "            elif l_conf >= self.thresholds['confidence_min']:\n",
    "                return {\n",
    "                    'prediction': l_pred,\n",
    "                    'confidence': l_conf * 0.9,\n",
    "                    'method': 'llm_confident',\n",
    "                    'reasoning': f'LLM mÃ¡s confiado: {l_conf:.2f} vs {t_conf:.2f}'\n",
    "                }\n",
    "            \n",
    "            else:\n",
    "                # Ambos poco confiados - usar transformer por ser especializado\n",
    "                return {\n",
    "                    'prediction': t_pred,\n",
    "                    'confidence': t_conf * 0.8,\n",
    "                    'method': 'transformer_fallback',\n",
    "                    'reasoning': 'Ambos poco confiados, prioridad a Transformer especializado'\n",
    "                }\n",
    "        \n",
    "        # Caso 4: NingÃºn modelo disponible\n",
    "        return {\n",
    "            'prediction': 'NO',\n",
    "            'confidence': 0.0,\n",
    "            'method': 'fallback',\n",
    "            'reasoning': 'NingÃºn modelo disponible, fallback conservador'\n",
    "        }\n",
    "    \n",
    "    def _get_strategy_used(self, transformer_result: Dict, llm_result: Dict) -> str:\n",
    "        \"\"\"Determinar quÃ© estrategia se utilizÃ³\"\"\"\n",
    "        t_avail = transformer_result.get('available', False)\n",
    "        l_avail = llm_result.get('available', False)\n",
    "        \n",
    "        if t_avail and l_avail:\n",
    "            return 'hybrid_ensemble'\n",
    "        elif t_avail:\n",
    "            return 'transformer_only'\n",
    "        elif l_avail:\n",
    "            return 'llm_only'\n",
    "        else:\n",
    "            return 'fallback'\n",
    "    \n",
    "    def batch_predict(self, texts: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        PredicciÃ³n en lote\n",
    "        \n",
    "        Args:\n",
    "            texts: Lista de textos a analizar\n",
    "            \n",
    "        Returns:\n",
    "            Lista de resultados\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        print(f\"ðŸ“Š Procesando {len(texts)} textos en modo hÃ­brido...\")\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            print(f\"   {i+1}/{len(texts)}: {text[:30]}...\")\n",
    "            result = self.ensemble_prediction(text)\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_performance(self, test_data: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluar rendimiento del sistema hÃ­brido\n",
    "        \n",
    "        Args:\n",
    "            test_data: Lista de diccionarios con 'text' y 'label'\n",
    "            \n",
    "        Returns:\n",
    "            MÃ©tricas de evaluaciÃ³n\n",
    "        \"\"\"\n",
    "        print(\"ðŸ“Š Evaluando rendimiento del sistema hÃ­brido...\")\n",
    "        \n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        \n",
    "        for item in test_data:\n",
    "            result = self.ensemble_prediction(item['text'])\n",
    "            pred = 1 if result['subtask_1_1']['prediction'] == 'YES' else 0\n",
    "            predictions.append(pred)\n",
    "            true_labels.append(item['label'])\n",
    "        \n",
    "        # Calcular mÃ©tricas\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        f1 = f1_score(true_labels, predictions)\n",
    "        precision = precision_score(true_labels, predictions)\n",
    "        recall = recall_score(true_labels, predictions)\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_score': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'total_samples': len(test_data)\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… EvaluaciÃ³n completada:\")\n",
    "        print(f\"   ðŸŽ¯ Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"   ðŸ“ˆ F1-Score: {f1:.4f}\")\n",
    "        print(f\"   ðŸ“Š Precision: {precision:.4f}\")\n",
    "        print(f\"   ðŸ”„ Recall: {recall:.4f}\")\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "print(\"âœ… Clase EXIST2025HybridSystem definida correctamente\")\n",
    "print(\"ðŸš€ Listo para crear instancia del sistema hÃ­brido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e61d47ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Cargando modelo afinado desde: ./model_hybryd\n",
      "   âœ… Tokenizer cargado.\n",
      "   âœ… Arquitectura SoftBinaryClassifier instanciada.\n",
      "   âœ… Pesos (safetensors) cargados en el modelo.\n",
      "   ðŸ’» Modelo listo en dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "# ðŸš€ INICIALIZAR SISTEMA HÃBRIDO\n",
    "# Instanciar pasando solo la ruta\n",
    "hybrid_system = EXIST2025HybridSystem(transformer_model_path=\"./model_hybryd\")\n",
    "\n",
    "# O instanciar pasando modelo y tokenizer ya cargados\n",
    "# hybrid_system = EXIST2025HybridSystem(transformer_model=model, transformer_tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44e52925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Analizando: Las mujeres no deberÃ­an trabajar fuera de casa...\n",
      "{'text': 'Las mujeres no deberÃ­an trabajar fuera de casa', 'subtask_1_1': {'prediction': 'YES', 'confidence': 0.8897032690048218, 'method': 'agreement', 'reasoning': 'Ambos modelos acordaron: YES'}, 'subtask_1_2': {'available': True, 'prediction': 'DIRECT', 'confidence': 0.8, 'raw_response': 'DIRECT', 'tokens_used': 509, 'model': 'mistral-nemo:latest'}, 'subtask_1_3': {'available': True, 'prediction': ['IDEOLOGICAL-INEQUALITY', 'STEREOTYPING-DOMINANCE'], 'confidence': 0.8, 'raw_response': 'Este tweet contiene dos tipos de sexismo:\\n\\nðŸ”¹ IDEOLOGICAL-INEQUALITY: El tweet niega la igualdad entre hombres y mujeres al decir que \"las mujeres no deberÃ­an trabajar fuera de casa\", lo que sugiere que las mujeres no son capaces o no deben hacer ciertas tareas.\\n\\nðŸ”¹ STEREOTYPING-DOMINANCE: El tweet tambiÃ©n perpetÃºa el estereotipo de roles de gÃ©nero tradicionales, donde se considera que las mujeres deben trabajar en el hogar y no fuera de Ã©l.', 'tokens_used': 921, 'model': 'mistral-nemo:latest'}, 'models_used': {'transformer': True, 'llm': True}, 'ensemble_strategy': 'hybrid_ensemble'}\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "text = \"Las mujeres no deberÃ­an trabajar fuera de casa\"\n",
    "resultado = hybrid_system.ensemble_prediction(text)\n",
    "print(resultado)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
